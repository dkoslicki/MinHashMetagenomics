 \documentclass[11pt]{amsart}

\usepackage[nocompress]{cite} % puts in-text citation numbers in order

\usepackage[normalem]{ulem}
%\usepackage{subfloat}
\usepackage{subfigure}
\usepackage{caption}
%\usepackage{subcaption}
\usepackage{amsaddr}
\usepackage{color}
%\usepackage{enumitem}
\usepackage{enumerate}
%\usepackage{natbib}
\usepackage{bbm}
\usepackage[mathscr]{eucal}
\usepackage{verbatim}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\newcommand*{\QEDB}{\hfill\ensuremath{\square}}%

\newcommand{\foot}[1]{\mbox{}\marginpar{\raggedleft\hspace{0pt}\tiny #1}}
%\newcommand{\foot}[1]{}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\sign}{{\rm sign}}
\newtheorem{definition}{Definition}[section]
\newtheorem{notation}{Notation}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{thma}{Theorem}
\renewcommand{\thethma}{\Alph{thma}}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}


\numberwithin{equation}{section}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\ones}{\mathbf{1}}
\DeclareMathOperator*{\argmin}{arg\,min}

%\usepackage{authblk}
%\usepackage{caption}
%\usepackage{subcaption}

\def\presuper#1#2%
  {\mathop{}%
   \mathopen{\vphantom{#2}}^{#1}%
   \kern-\scriptspace%
   #2}

\DeclareMathOperator*{\IGP}{\presuper{IGP}{\mathit{A}}}
\DeclareMathOperator*{\Tri}{\presuper{Tri}{\mathit{A}}}
\DeclareMathOperator*{\MRS}{{\rm MRS}}
\DeclareMathOperator*{\SSS}{{\rm SS}}
%\DeclareMathOperator*{\NS}{NS}
\newcommand{\NS}{NS}

\newcommand{\mn}{\color{blue}}

\newcommand{\themethod}{containment min hash }
\DeclareMathOperator{\Jac}{{\rm Jac}}

%%%%%%%%%%%%%%%%%%%%%%%%%%
% All the data for the paper
\newcommand{\ClassicalConceptual}{\protect \input{Data/ClassicalConceptual.txt}}
\newcommand{\ContainmentConceptual}{\protect \input{Data/ContainmentConceptual.txt}}
\newcommand{\FoundOrganismContainment}{\protect \input{Data/FoundOrganismContainment.txt}}
\newcommand{\FoundOrganismJaccard}{\protect \input{Data/FoundOrganismJaccard.txt}}
\newcommand{\FoundOrganismName}{\protect \input{Data/FoundOrganismName.txt}}
\newcommand{\MeanCoverage}{\protect \input{Data/MeanCoverage.txt}}
\newcommand{\NumReadsAligned}{\protect \input{Data/NumReadsAligned.txt}}
\newcommand{\SimulatedBiologicalDataksize}{\protect \input{Data/SimulatedBiologicalDataksize.txt}}
\newcommand{\SimulatedBiologicalDataNumGenomes}{\protect \input{Data/SimulatedBiologicalDataNumGenomes.txt}}
\newcommand{\SimulatedBiologicalDataNumReads}{\protect \input{Data/SimulatedBiologicalDataNumReads.txt}}
\newcommand{\SimulatedBiologicalDataNumReplicates}{\protect \input{Data/SimulatedBiologicalDataNumReplicates.txt}}
\newcommand{\SimulatedBiologicalDatap}{\protect \input{Data/SimulatedBiologicalDatap.txt}}
\newcommand{\SimulatedBiologicalDataRelSize}{\protect \input{Data/SimulatedBiologicalData_rel_size.txt}}
\newcommand{\SimulatedBiologicalDataSmallKsize}{\protect \input{Data/SimulatedBiologicalData_small_ksize.txt}}
\newcommand{\SimulatedBiologicalDataSmallNumGenomes}{\protect \input{Data/SimulatedBiologicalData_small_NumGenomes.txt}}
\newcommand{\SimulatedBiologicalDataSmallNumReads}{\protect \input{Data/SimulatedBiologicalData_small_NumReads.txt}}
\newcommand{\SimulatedBiologicalDataSmallNumReplicates}{\protect \input{Data/SimulatedBiologicalData_small_NumReplicates.txt}}
\newcommand{\SimulatedBiologicalDataSmallP}{\protect \input{Data/SimulatedBiologicalData_small_p.txt}}
\newcommand{\SimulatedBiologicalDataSmallRelSize}{\protect \input{Data/SimulatedBiologicalData_small_rel_size.txt}}
\newcommand{\SyntheticDataClassic}{\protect \input{Data/SyntheticDataClassic.txt}}
\newcommand{\SyntheticDataContainment}{\protect \input{Data/SyntheticDataContainment.txt}}
\newcommand{\WindowSize}{\protect \input{Data/WindowSize.txt}}

% % % % % % % % % % % % % % % %
% Potential reviewers:
% 

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\begin{document}

\title[Improving Min Hash]{Improving Min Hash for Metagenomic Taxonomic Profiling} %Inside the square brackets is the running head


\author{Hooman Zabeti${}^{1}$, David Koslicki${}^{1*}$}
\address{${}^1$ Mathematics Department, Oregon State University, Corvallis, OR.}
\thanks{${}^*$ Corresponding Author: \url{david.koslicki@math.oregonstate.edu}}





\date{\today}
\begin{abstract}
Abstract here.  \\\\
\smallskip
\noindent \textsc{Keywords}: \emph{Min hash, k-mins sketch, metagenomics, taxonomic profiling, taxonomic classification, Jaccard index, containment}.
\end{abstract}
\maketitle


\section{Introduction}
\begin{enumerate}
\item Min hash recently has been used to great success on biological data
\item Mash, Sourmash, etc.
\item originally designed for sets of relatively similar size and appreciable intersection size
\item metagenomic taxonomic profiling the setup is different: many relatively small database entries, one very large metagenomic sample, very small intersection sizes in general
\item we modify the min hash paradigm to this particular situation so it can handle a sample of much greater size than the reference database entries.
\end{enumerate}

Min hash is great at comparing sets of similar size. When one set is much larger than the other, the Jaccard index is going to be smaller, which is precisely the case when you need a lot of hashes to get a good estimate (by the Chernoff bounds). In metagenomics, the typical paradigm is one/few very large set(s) (the metagenomic sample(s)) call it $B$ and a bunch of small reference/database sets (call one $A$). Taking the classical min hash approach means sampling from $A\cup B$. Part a) of Figure \ref{fig:Conceptual} demonstrates such a situation while sampling 100 random points of $A\cup B$ and leads to \input{Data/ClassicalConceptual.txt}points lying in $A\cap B$. On the other hand, if we sample from just $A$ (instead of $A\cup B$) and have some way to test if a point $x$ is in $A\cap B$, we would get a much better estimate of $|A\cap B|$. Part b) of Figure \ref{fig:Conceptual} demonstrates this approach while sampling only 50 points from $A$ and finds \input{Data/ContainmentConceptual.txt}points lying in $A\cap B$. They key to our approach is that the membership test $x\in A\cap B$ can be efficiently performed with a bloom filter of $B$. We analyze the time and space complexity, as well as the accuracy of this approach and find that for parameters typically used in metagenomics, our proposed approach is faster, uses less space, and is more accurate than the classical min hash approach.


\begin{figure}[!h]%
\begin{center}
\includegraphics[width=3.15in,trim={0 0 0 0in},clip]{Figs/ClassicalConceptual.png}%
\hspace{1ex}
\includegraphics[width=3.0in,trim={0 0 0 0in},clip]{Figs/ContainmentConceptual.png}
\end{center}
\caption{Conceptual comparison of classical min hash to the proposed containment approach when estimating the Jaccard index of very different sized sets. a) Sampling 100 points from $A\cup B$ (as is done in the classical min hash approach) leads to finding only \ClassicalConceptual elements in $A\cap B$. b) Sampling just 50 points of $A$ and testing if a point $x\in A\cap B$, finds \ContainmentConceptual elements in $A\cap B$. This latter approach will be seen to lead to a better estimate of the Jaccard index. }
\label{fig:Conceptual}%
\end{figure}





\section{Methods}
Definitions, derivation of mathematical results here.
\emph{Need to carefully choose notation so we don't repeat the usage of $X$}
\subsection{Definitions}
\begin{enumerate}
\item Definitions of database entries, query sample, k-mer size, note size disparity
\item $k$-mer size, genome $=$ set of all $k$-mers from some genomes, give typical size. Sample is all $k$-mers from some large set of short reads, give typical size
\end{enumerate}

\subsection{Classic min hash}
\textit{describe classic approach.}
Pick at random some number of hashes, compute minimum of said hash function on pair of sets, look for collision, prob. of collision is Jaccard.
In the MinHash technique with different $k$ hash functions, define
$$X_i=\left\{
\begin{array}{lll}
1 &h^{(i)}_{min}(A)=h^{(i)}_{min}(B)\\
0& o.w
\end{array}
\right.$$
Which give 
$E(X_i)= \dfrac{|A\cap B|}{|A\cup B|}=J(A,B)$.
Hence for 
$X^k=\sum\limits_{i=1}^k X_i$,
the expectation is given by % $X^k$ would be $k J(A,B)$ 
$E(X^k)=k J(A,B)$.

\subsection{Min hash via containment}
\textit{In this subsection, sketch the general idea of what we are doing.}
Compute Bloom filter of larger set, randomly select some number of hashes, compute minimums of said hash function on the smaller set, check if minimum is in bloom filter. Prob(yes) is containment/coverage index (intersection over smaller set).


\subsection{Analytic comparison of classical min hash to containment min hash.}
\subsubsection{Chernoff bound estimates}
\begin{definition}[Give reference]
\label{def:GeneralChernoffBounds}
Let $X$ be a random variable such that $E(X)=\mu,\quad 0<\delta<1$. Then probability of relative error/deviation $\delta$ from mean $\mu$ is given by
\begin{align*}
%P(X\leq(1-\delta)\mu\leq e^{-\delta^2\mu/2},\quad P(X\geq(1+\delta)\mu)\leq e^{-\delta^2\mu/3} \\
%\Rightarrow\quad &
P\left(\left|\frac{X-\mu}{\mu}\right|\geq\delta\right)\leq2e^{-\delta^2\mu/3}
\end{align*}
\end{definition}

Applying this to the classic min hash approach
$$P\left( \left|\dfrac{\frac{X^k}{k}-J(A,B)}{J(A,B)}\right|\geq\delta\right)\leq 2e^{-\delta^2kJ(A,B)/3}$$
Now let $k_J$ be the number of hash functions and $t$ be the confident of the Chernoff bound( $k_J:=k,\quad t:=2e^{-\delta^2k_JJ(A,B)/3}$), therefore
$$k_J=\dfrac{-3\ln(t/2)|A\cup B|}{\delta^2 |A\cap B|}$$
In our new approach, for a bloom filter $(B)_b$ of a set $B$ with false positive rate of $p$, let 
$$X_i=\left\{
\begin{array}{ll}
1&\text{ if } a\in(B)_b\text{ for } a=argmin\{h^{(i)}{(e)}:e\in A\}\\
0& o.w
\end{array}
\right.$$

i.e. $X_i$ will determine membership of the element of $A$ with the smallest image under hash function $h^{(i)}$ in $B$.(So take elem of $A$ that hashes to smallest value under $h^{(i)}$ and check for memberships in B). Therefore $E(X_i)=\dfrac{|A\cap B|}{|A|}+p$. Let 
$X^k=\sum\limits_{i=1}^k X_i$, hence $ E(X^k)=k\dfrac{|A\cap B|}{|A|}+kp$. Define 
$c:= \dfrac{|A\cap B|}{|A|}$,  $J:= \dfrac{|A\cap B|}{|A\cup B|} $ and
$\mu:=k c+k p$ (i.c $\mu=E(X^k)$). Then
\begin{align*}
P\left(
\left|\dfrac{\left(\frac{X^k}{k}-p\right)-c}{c}\right|\geq \delta\right)
&=P\left(X^k\geq\left(1+\dfrac{kc\delta}{\mu}\right)\mu\right) +P\left(X^k\leq \left(1-\dfrac{kc\delta}{\mu}\right)\mu\right)\\
&=P\left( X^k\geq\left( 1+\left(\dfrac{c}{c+p}\right)\delta\right)\mu\right) + P\left(X^k\leq\left(1-\left(\dfrac{c}{c+p}\right)\delta\right)\mu\right)\\
&\leq 2e^{-\left(\frac{c}{c+p}\right)^2\delta^2k(c+p)/3}
\end{align*}
So for $t:=2e^{-\left(\frac{c}{c+p}\right)^2\delta^2k(c+p)/3}$, the desired Chernoff bound, let $k_c=k$ be the number of hash functions which is required to achieve this bound. Then 
$$k_c=\dfrac{-3(c+p)\ln(t/2)}{c^2\delta^2}$$
%%%% Fraction of sample comprised of genome A
Define $c_{est}:=\frac{X^k}{k}-p$. Similarly 
\begin{align*}
 P\left(
\left|\dfrac{\frac{|A|c_{est}}{|B|}-\frac{|A\cap B|}{|B|}}{\frac{|A\cap B|}{|B|}}\right|\geq \delta\right)
&=P\left(
\left|\dfrac{\frac{|A|c_{est}}{|B|}-\frac{|A|c}{|B|}}{\frac{|A|c}{|B|}}\right|\geq \delta\right)\\
&=P\left(
\left|\dfrac{\left(\frac{X^k}{k}-p\right)-c}{c}\right|\geq \delta\right)\\
&\leq 2e^{-\left(\frac{c}{c+p}\right)^2\delta^2k(c+p)/3}
\end{align*}
%%%% Jaccard estimation
For $c_{est}$, our estimation of containment, define $J_{est}:=\frac{|A|c_{est}}{|A|+|B|-|A|c_{est}}$. Therefore,
\begin{align*}
P\left(\left|\frac{J_{est}-J}{J}\right|\geq \delta\right) &= P(J_{est}\geq (1+\delta)J)+P(J_{est}\leq (1-\delta)J)\\
&= P\left(\frac{|A|c_{est}}{|A|+|B|-|A|c_{est}}\geq (1+\delta)J\right)+P\left(\frac{|A|c_{est}}{|A|+|B|-|A|c_{est}}\leq (1-\delta)J\right)\\
&= P\left(c_{est}\geq\frac{ (1+\delta)J(|A|+|B|)}{|A|(1+(1+\delta)J)} \right)+P\left(c_{est}\leq\frac{ (1-\delta)J(|A|+|B|)}{|A|(1+(1-\delta)J)} \right)\\
&= P\left(c_{est}\geq\frac{ (1+\delta)(\frac{|A|c}{|A\cup B|})(|A|+|B|)}{|A|(\frac{|A\cup B|+(1+\delta)|A \cap B|}{|A\cup B|})} \right)+P\left(c_{est}\leq\frac{ (1-\delta)(\frac{|A|c}{|A\cup B|})(|A|+|B|)}{|A|(\frac{|A\cup B|+(1-\delta)|A \cap B|}{|A\cup B|})} \right)\\
&= P\left(c_{est}\geq\frac{ (1+\delta)(|A|+|B|)}{|A\cup B|+(1+\delta)|A \cap B|} c \right)+P\left(c_{est}\leq\frac{ (1-\delta)(|A|+|B|)}{|A\cup B|+(1-\delta)|A \cap B|} c \right)\\
&= P\left(c_{est}\geq\frac{ (1+\delta)(|A \cup B|+|A \cap B|)}{|A\cup B|+(1+\delta)|A \cap B|} c \right)+P\left(c_{est}\leq\frac{ (1-\delta)(|A \cup B|+|A \cap B|)}{|A\cup B|+(1-\delta)|A \cap B|} c \right)\\
&= P\left(c_{est}\geq\left(1+\frac{\delta |A \cup B|}{|A\cup B|+(1+\delta)|A \cap B|}\right) c \right)\\
&+P\left(c_{est}\leq\left(1-\frac{ \delta |A \cup B|}{|A\cup B|+(1-\delta)|A \cap B|}\right) c \right)\\
\end{align*}
define $\delta\rq{}= \frac{\delta |A \cup B|}{|A\cup B|+(1+\delta)|A \cap B|}$ and $\delta\rq{}\rq{}=\frac{\delta |A \cup B|}{|A\cup B|+(1-\delta)|A \cap B|}$. Therefore,
\begin{align*}
P\left(\left|\frac{J_{est}-J}{J}\right|\geq \delta\right) &\leq e^{-(\frac{c}{c+p})^2\delta\rq{}k(c+p)/3}+e^{-(\frac{c}{c+p})^2\delta\rq{}\rq{}k(c+p)/2}\\
&\leq 2e^{-(\frac{c}{c+p})^2\delta\rq{}k(c+p)/3}
\end{align*}
Let $k_{est}:=k$ be the number of hash functions which is required to achieve a desire confident $t:=2e^{-(\frac{c}{c+p})^2\delta\rq{}k(c+p)/3}$. Therefore
 $$k_{est} =\frac{-3(c+p)ln(t/2)\left(|A\cup B|+(1+\delta)|A\cap B|\right)^2}{c^2\delta^2|A\cup B|^2 } $$
%%%%%%%%%%%%%%%%%% Comparison of #hashes%%%%%%%%%%%%%%%%%%
\subsubsection{Number of hashes required}
By comparing the ratio of the number of hash functions required for the Jaccard approach and the containment approach (our new approach), 
$$\dfrac{k_J}{k_c}=\dfrac{-2\ln(t/2)}{\delta^2 J} \dfrac{c^2\delta^2}{-2(c+p)\ln(t/2)}=\dfrac{c}{J}\left(\dfrac{c}{c+p}\right)$$
When $p=0.01$ and $\geq0.01$, $ \dfrac{c}{c+p}\geq.90$
$$\dfrac{k_J}{k_c}\geq .9\frac{c}{J}=.9 \dfrac{|A\cup B|}{|A|}=.9 \dfrac{(|A|+|B|-|A\cap B|)}{|A|}\geq .9 \dfrac{|B|}{|A|}$$
Therefore to obtain a specific accuracy and particular Chernoff boundary, containment approach needs $\frac{|B|}{|A|}$ times fewer hash functions than the Jaccard approach.($k_J \approx\frac{|B|}{|A|}k_c$)\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!h]%
\begin{center}
\includegraphics[width=3.0in,trim={0 0 0 0in},clip]{Figs/1010.png}%
\hspace{1ex}
\includegraphics[width=3.0in,trim={0 0 0 0in},clip]{Figs/1090.png}\\
\includegraphics[width=3.0in,trim={0 0 0 0in},clip]{Figs/20010.png}%
\hspace{1ex}
\includegraphics[width=3.0in,trim={0 0 0 0in},clip]{Figs/20090.png}
\end{center}
\caption{Explanation+ example }
\label{fig:DeltaK}%
\end{figure}
%%%%%%%%%%%%%%%%%%% Time Complexity%%%%%%%%%%%%%%%%%%%%
\subsubsection{Time complexity}
Both approaches linear in the number of hashes. (So containment approach is $m$ times faster for typical set sizes).
%%%%%%%%%%%%%%%%%% Space Complexity%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Space complexity (all with examples of the numbers in practice)}
Even though we must form a bloom filter for a metagenomic sample (which can potentially contain many billions of $k$-mers), due to the many fewer hashes we must store (orders of magnitude fewer) there actually turns out to be a cost savings due to the size of training databases.

Let $G$ be the number of training genomes utilized, $M$ the maximum number of $k$-mers in any of the training genomes. Let $N$ be the number of $k$-mers in the sample. Typical values are XXXX. Recall that $k_c$ and $k_J$ are the number of hashes required in the containment approach and classic approach respectively.

In practice, we see $48$ bits per hash seen in practice, so the total number of bits used by the classical approach is given by
\begin{align*}
%Jaccard &: \approx 48 \text{ bits/hash seen in practice}  \Rightarrow 48 k_c m (\#genomes)=: S_J\\
S_J = 48k_J\cdot  G
\end{align*}


Typical bloom filter implementations use approximately $1.44\log_2(1/p)$ bits per item where $p$ is the false positive rate. Hence, when $p = 0.01$, the total number of bits used by the proposed containment approach is give by
\begin{align*}
%Containment &:  48 k_c (\#genomes)+1.44\log_2(1/p) |B| \\
%&\cong 48 k_c (\#genomes)+9.6|B| = 48 k_c(\#genomes)+ 9.6 |A| m=: S_c
S_c = 48k_c \cdot G + 9.6N
\end{align*}

It is reasonable to assume that in practice, the number of $k$-mers in the sample $S$ is some (large) multiple $m$ of the largest number of $k$-mers found in any one of the training genomes $M$: $S \leq m\cdot  M$. Hence, comparing the ration of the size requirements of the approaches, we obtain:
\begin{align*}
\dfrac{S_c}{S_J} = \dfrac{1}{5}\dfrac{M}{G\cdot  k_c}+\dfrac{1}{m}
\end{align*}

Typically $G$ is greater than 30 thousand, $M$ is less than 4.3 million, $k_c=500$ (change this to confidence and delta), and $m\geq 200$. Hence, $\dfrac{S_c}{S_J}\leq 0.06$, so the containment approach actually uses significantly less space.
% Even though the bloom filter is large, we have m times fewer hashes per genome and $\approx 30000 $ genomes.

\textit{Include a plot of tradeoff in sizes (metagenome $k$-mers vs number of training genomes)}


\section{Results}
In this section, we compare classic min hash to the proposed method.
\subsection{Synthetic data}
\label{section:SyntheticData}
Here we illustrate the improved accuracy of \themethod over classical min hash in estimating the Jaccard index. To that end, we generated two random strings $w_A$ and $w_B$ on the alphabet $\{A,C,T,G\}$. We set $|w_A|= 10,000$ and $|w_B| = 15$ to simulate the situation of interest where one wishes to estimate the Jaccard index of two sets of very different size. We then appended a common string $w_C$ of increasing length to each of $w_A$ and $w_B$ so that $\Jac_k(w_Aw_C, w_Bw_C)$ ranges between 0 and 1. We picked the $k$-mer size of $11$ and utilized a signature size of 100. Figure \ref{fig:TrueVsEstimate} depicts the comparison of \themethod with the classical min hash Jaccard estimate on this data and effectively illustrates the results in section \ref{section:ChernoffBounds} which proved that the containment approach has a higher probably of being closer to the true Jaccard than the classic approach. The mean and variance of the classic min hash approach on this data was \SyntheticDataClassic while using the containment approach was \SyntheticDataContainment demonstrating a substantial decrease in variance. This improved variance was observed over a range of $k$-mer sizes, number of hashes, and lengths of input strings.


\begin{figure}[!h]%
\begin{center}
\includegraphics[width=3.25in,trim={0 0 0 0in},clip]{Figs/TrueVsEstimate.png}%
\includegraphics[width=3.25in,trim={0 0 0 0in},clip]{Figs/ContainmentTrueVsEstimate.png}
\end{center}
\caption{Comparison of \themethod to the classical min hash estimate of the Jaccard index on synthetic data. Each method utilized the 100 smallest hashes of the murmer3 hash function on the $11$-mers of two randomly generated strings with sizes 10,000 and 15 respectively after appending a common substring of increasing size. a) Classical min hash estimate of the Jaccard index. b) The proposed \themethod method on the same data.}
\label{fig:TrueVsEstimate}%
\end{figure}


\subsection{Simulated biological data}
To demonstrate the exponential improvement of \themethod over classical min hash for increasing sample sizes, we contrast here the mean relative performance of the classical min hash estimate to the containment approach on simulated biological data. We utilized GemSIM \cite{mcelroy2012gemsim} to simulate two sets of metagenomic data. The first set had an average number of $k$-mers in the sample was only \SimulatedBiologicalDataSmallRelSize of the size of the average number of $k$-mers in the genomes used to simulate the data. The second set had an average number of $k$-mers in the sample equal to \input{Data/SimulatedBiologicalData_rel_size.txt}of the average size of the number of $k$-mers in the genomes used to simulate the data. As demonstrated in Section \ref{section:ChernoffBounds} we expect that once the number of $k$-mers in the sample is large in comparison to the number of $k$-mers used to simulate the data, the containment approach will give an exponentially better estimate of the Jaccard index in comparison to the classical min hash approach. Figure \ref{fig:SimulatedBiologicalData} depicts the relative error of the classic min hash approach and the containment approach on these two sets of simulated data. Observe that the containment approach has significantly less error when, as is commonly seen in practice, the number of $k$-mers in the sample is appreciable in comparison to the number of $k$-mers in a given reference organism. As demonstrated in section \ref{section:ChernoffBounds}, this improvement of the containment approach over the classic approach continues to grow as the metagenome size grows in relation to the reference genome sizes.

For the first set of simulated data, we used GemSIM to simulate \SimulatedBiologicalDataSmallNumReads reads from \SimulatedBiologicalDataSmallNumGenomes randomly selected bacterial genomes for the $k$-mer size $k=\SimulatedBiologicalDataSmallKsize$. We then repeated this \SimulatedBiologicalDataSmallNumReplicates times. A false positive rate of \SimulatedBiologicalDataSmallP was used for the false positive rate of the bloom filter used for the containment approach.

For the second set of simulated data, we used GemSIM to simulate \SimulatedBiologicalDataNumReads reads from \SimulatedBiologicalDataNumGenomes randomly selected bacterial genomes for the $k$-mer size $k=\SimulatedBiologicalDataksize$. We then repeated this \SimulatedBiologicalDataNumReplicates times. A false positive rate of \SimulatedBiologicalDatap was used for the false positive rate of the containment approach.

\begin{figure}[!h]%
\begin{center}
\includegraphics[width=3.25in,trim={0 0 0 0in},clip]{Figs/SimulatedBiologicalData_small.png}%
\includegraphics[width=3.25in,trim={0 0 0 0in},clip]{Figs/SimulatedBiologicalData.png}
\end{center}
\caption{Comparison of relative error of \themethod to the classical min hash estimate of the Jaccard index on simulated biological data. a) On \SimulatedBiologicalDataSmallNumReplicates replicates of samples consisting of \SimulatedBiologicalDataSmallNumGenomes genomes with only \SimulatedBiologicalDataSmallNumReads reads. b) On \SimulatedBiologicalDataNumReplicates replicates of samples consisting of \SimulatedBiologicalDataNumGenomes genomes with \SimulatedBiologicalDataNumReads reads.
}
\label{fig:SimulatedBiologicalData}%
\end{figure}

\subsection{Real biological data}

Real metagenomes contain many magnitudes more $k$-mers than those found in any reference organisms \cite{} which indicates the advantage of utilizing the proposed containment approach to the classical min hash estimate of the Jaccard index. To evaluate the utility of the containment min hash approach on real biological data, we analyzed a subset of DNA generated by the study in \cite{howe2014tackling} consisting of those reads contained in the sample 4539585.3.fastq. This sample consisted of 25.4M reads with average length of 65bp. We formed a bloom filter consisting of all 21-mers of this sample and formed sketches of size 500 from 4,798 viral genomes obtained from NCBI. Utilizing the proposed containment min hash approach, we found the largest containment index between the reference viral metagenomes and the sample to be \FoundOrganismContainment for the virus \textit{\FoundOrganismName}which corresponds to a Jaccard index of \FoundOrganismJaccard \unskip. As demonstrated in section \ref{section:ChernoffBounds} we can be XX\% sure that the true Jaccard index between this genome and the sample is within a relative error of XX\% of the true Jaccard index value.  If we were to use the classical min hash approach, the Chernoff bounds dictate that we would min hash sketches of size XXX to achieve this same confidence bound on the relative error.

To evaluate if this extremely low-abundance organism is actually present in the sample, we utilized the SNAP alignment tool \cite{zaharia2011faster} to align the sample to the \textit{\FoundOrganismName} genome. The script \textit{MakeCoveragePlot.sh} provides the exact commands and parameters used to perform the alignment. We found that \NumReadsAligned reads aligned with a MAPQ score above 20 (XXgrab valueXX). The coverage of the viral genome is depicted in Figure \ref{fig:ViralCoverage} using a square-root scale and a window size of \WindowSize\unskip. 
%Even though the coverage was quite low (average per-window coverage was \input{Data/MeanCoverage.txt}\unskip X), 
These high-quality mapped reads to such a small genome lends evidence to support the claim that this particular virus is actually present in the sample metagenome.

\begin{figure}[!h]%
\begin{center}
\includegraphics[width=3.25in,trim={0 0 0 0in},clip]{Figs/CoveragePlot.png}%
\end{center}
\caption{Plot of the real metagenomic sample alignment coverage to the virus \textit{\FoundOrganismName} detected by the proposed containment min hash approach. A total of \NumReadsAligned reads aligned with a MAPQ score above 20 (XXgrab valueXX) using the SNAP aligner \cite{zaharia2011faster}. A square root scale and a window size of \WindowSize was used for the plot, resulting in an average per-window coverage of \MeanCoverage \unskip X.
}
\label{fig:ViralCoverage}%
\end{figure}

\section{Discussion}


\clearpage
\bibliography{library}{}
\bibliographystyle{abbrv}

\end{document}

